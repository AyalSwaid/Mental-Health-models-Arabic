{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc758f64-3f6a-4e28-8dea-48cfb43eaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, fbeta_score, accuracy_score\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    fbeta_score,\n",
    "    accuracy_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "get_time = lambda: f\"[{datetime.now():%H:%M:%S}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51268b36-164b-42ba-813a-2924d2ee1ec1",
   "metadata": {},
   "source": [
    "# Define some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2b912-e51f-4ba6-abac-57009a6abd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def classification_report_with_f2(y_true, y_pred, label_names=None, beta=2.0, digits=4):\n",
    "    # 1) Fix label set and order explicitly\n",
    "    labels = sorted(np.unique(np.concatenate([y_true, y_pred])))\n",
    "\n",
    "    # 2) Consistent zero_division across all calls\n",
    "    zd = 0\n",
    "\n",
    "    # Per-class metrics\n",
    "    p, r, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=zd\n",
    "    )\n",
    "    f2 = fbeta_score(y_true, y_pred, beta=beta, labels=labels, average=None, zero_division=zd)\n",
    "\n",
    "    # Macro and weighted (match classification_report rows)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=zd\n",
    "    )\n",
    "    p_w, r_w, f1_w, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, average=\"weighted\", zero_division=zd\n",
    "    )\n",
    "    f2_macro = fbeta_score(y_true, y_pred, beta=beta, labels=labels, average=\"macro\", zero_division=zd)\n",
    "    f2_weighted = fbeta_score(y_true, y_pred, beta=beta, labels=labels, average=\"weighted\", zero_division=zd)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Build a DataFrame like classification_report + F2\n",
    "    index = [str(l) for l in labels] if label_names is None else list(label_names)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"precision\": p,\n",
    "            \"recall\": r,\n",
    "            \"f1-score\": f1,\n",
    "            \"f2-score\": f2,\n",
    "            \"support\": support.astype(int),\n",
    "        },\n",
    "        index=index\n",
    "    )\n",
    "    # print(f2)\n",
    "    print(\"\\n\", df.round(3))\n",
    "\n",
    "    return df.round(4), f2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a4e3d-c99e-4131-a898-8e41700d5ce8",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d804d-270a-47ad-a9cb-0b01e60b6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "load_dir = \"./saved_objects\"\n",
    "\n",
    "pkl_train_conv = pickle.load(open(os.path.join(load_dir, \"train_conv.pkl\"), 'rb'))\n",
    "pkl_test_conv = pickle.load(open(os.path.join(load_dir, \"test_conv.pkl\"), 'rb'))\n",
    "pkl_unlabeled_msgs = pickle.load(open(os.path.join(load_dir, \"unlabeled_msgs.pkl\"), 'rb'))\n",
    "pkl_train_msgs = pickle.load(open(os.path.join(load_dir, \"train_msgs.pkl\"), 'rb'))\n",
    "pkl_test_msgs = pickle.load(open(os.path.join(load_dir, \"test_msgs.pkl\"), 'rb'))\n",
    "pkl_pretrain_messages_df = pickle.load(open(os.path.join(load_dir, \"pretrain_messages_df.pkl\"), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f86c3-f6c8-48bc-b8b8-d050635f5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARABIC_LEXICON_PATH = \"../new_arabic_lexicon_17_07.csv\"\n",
    "needed_categories = [\"Past suicidal history\", \"Family suicide history\", \"Suicidal ideation\", \"Hopelessness\", \"Deliberate self harm\", \"Perceived burdensomeness\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9e6e2-509e-4cba-b105-2601c370e4fd",
   "metadata": {},
   "source": [
    "# Init DataSets, DataLoaders, optimizer and model then train our pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8791f-340f-4f0b-876f-0f287f3f14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"./pretrain/mlm & reg arabertv02 large (lr=7e-6)/bert_pretrained\"\n",
    "\n",
    "tokenizer_id = \"aubmindlab/bert-large-arabertv02\"\n",
    "\n",
    "conversations_path = \"arabic_conversations.csv\"\n",
    "messages_path = \"arabic_messages.csv\"\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "num_labels = 2\n",
    "max_length = 512\n",
    "test_size = 0.3\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "learning_rate = 5e-6\n",
    "log_every_n = 200\n",
    "\n",
    "\n",
    "max_f2_score = 0 # best f2 score over the epochs\n",
    "best_model = None # best model over the epochs (by f2 score)\n",
    "best_model_epoch = 0 # what epoch was the best model\n",
    "last_f2_value = 0 # value of f2 at the end of training (all epochs)\n",
    "\n",
    "current_run_time = datetime.now().strftime(\"%d-%m-%Y-_%H-%M\")\n",
    "save_dir = f\"./BERT_saves/run - {current_run_time}\"\n",
    "\n",
    "all_evaluation_epochs = []  # list of eval metrics for each epoch\n",
    "\n",
    "\n",
    "class SaharDataset(Dataset):\n",
    "    def __init__(self, conversations_df, messages_df, tokenizer, max_length=max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Step 1: Filter messages to seekers only\n",
    "        seeker_messages = messages_df[messages_df['seeker']]\n",
    "        print(f\"seeker_messages: {len(seeker_messages)}, unique_seeker_messages_engagment_id: {seeker_messages.engagement_id.unique().shape}\")\n",
    "\n",
    "        # Step 2: Group by engagement_id and concatenate text\n",
    "        grouped = seeker_messages.groupby('engagement_id')['text'].apply(lambda texts: '. '.join(texts))\n",
    "        print(f\"convs: grouped:{len(grouped)}, unique_msgs_engagment_id: {messages_df.engagement_id.unique().shape}, total convs:\", len(conversations_df))\n",
    "        \n",
    "        # Step 3: Merge with conversations to get labels\n",
    "        merged = pd.merge(grouped, conversations_df[['engagement_id', 'gsr']], on='engagement_id')\n",
    "\n",
    "        self.labels = merged['gsr'].tolist()\n",
    "\n",
    "        # Step 4: Tokenize all texts now\n",
    "        self.encodings = tokenizer(\n",
    "            merged['text'].tolist(),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.texts = merged['text'].values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx],\n",
    "            \"text\": self.texts[idx]\n",
    "        }\n",
    "\n",
    "        \n",
    "def create_dataloaders(conversations_df, messages_df, tokenizer, batch_size=16, seed=42, test_size=0.2, max_length=max_length):\n",
    "    global pkl_train_conv\n",
    "    global pkl_test_conv\n",
    "\n",
    "    train_conv, test_conv = pkl_train_conv, pkl_test_conv\n",
    "    \n",
    "    # extract messages based on engagement_id from conversations splits\n",
    "    train_ids = set(train_conv['engagement_id'].values)\n",
    "    test_ids = set(test_conv['engagement_id'].values)\n",
    "\n",
    "    \n",
    "    train_msgs = messages_df[messages_df['engagement_id'].isin(train_ids)]\n",
    "    test_msgs = messages_df[messages_df['engagement_id'].isin(test_ids)]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SaharDataset(train_conv, train_msgs, tokenizer, max_length=max_length)\n",
    "    test_dataset = SaharDataset(test_conv, test_msgs, tokenizer, max_length=max_length)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "    \n",
    "def evaluate(model, data_loader, device, curr_epoch):\n",
    "    \"\"\"\n",
    "    Iterate over evaluation data loader and print its evaluation results along with \n",
    "    current epoch number, this function is called after each training epoch to track performance over epochs\n",
    "    \"\"\"\n",
    "    global max_f2_score\n",
    "    global best_model\n",
    "    global best_model_epoch\n",
    "    global last_f2_value\n",
    "    global epochs\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    print(f\"Epoch: {curr_epoch}\")\n",
    "    metrics_df, f2 = classification_report_with_f2(all_labels, all_preds)\n",
    "    \n",
    "    all_evaluation_epochs.append(metrics_df.iloc[1, :].to_dict())\n",
    "\n",
    "    if max_f2_score < f2: # save best version\n",
    "        max_f2_score = f2\n",
    "        best_model = model\n",
    "        best_model_epoch = curr_epoch\n",
    "\n",
    "    if curr_epoch == epochs:\n",
    "        last_f2_value = f2\n",
    "        \n",
    "    return f2\n",
    "\n",
    "\n",
    "def load_model(model_dir = \"./training_test\"):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    epochs=3,\n",
    "    scheduler=None,\n",
    "    log_every_n=10\n",
    "):\n",
    "    model.to(device)\n",
    "    global current_run_time\n",
    "    global last_f2_value\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            if (batch_idx + 1) % log_every_n == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}] Batch [{batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        f2 = evaluate(model, test_loader, device, curr_epoch=epoch+1)\n",
    "        model.save_pretrained(f\"{save_dir}/epoch={epoch+1}_f2={f2:.3f}__{current_run_time}\")\n",
    "\n",
    "    \n",
    "    # best_model_dir = \"best_model_save\"\n",
    "    if best_model is not None:\n",
    "        best_model.save_pretrained(f\"{save_dir}/best f2={max_f2_score:.3f}_epoch={best_model_epoch}_{current_run_time}\")\n",
    "    # tokenizer.save_pretrained(f\"{save_dir}/best f2={max_f2_score}_epoch={best_model_epoch}_{current_run_time}\")\n",
    "    return model\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=num_labels)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# return\n",
    "print(\"Loading conversations and messages...\")\n",
    "conversations = pd.read_csv(conversations_path)\n",
    "messages = pd.read_csv(messages_path)\n",
    "\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, test_loader = create_dataloaders(conversations, messages, tokenizer, batch_size=batch_size, seed=seed, test_size=test_size, max_length=max_length)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "model = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    scheduler=scheduler,\n",
    "    log_every_n=log_every_n\n",
    ")\n",
    "\n",
    "train_finish_save_dir = f\"{save_dir}/last_epoch={epochs}_f2={last_f2_value:.3f}\"\n",
    "\n",
    "\n",
    "model.save_pretrained(train_finish_save_dir)\n",
    "tokenizer.save_pretrained(train_finish_save_dir)\n",
    "\n",
    "all_evaluation_epochs_df = pd.DataFrame(all_evaluation_epochs)\n",
    "all_evaluation_epochs_df['epoch'] = np.arange(1, len(all_evaluation_epochs_df)+1)\n",
    "all_evaluation_epochs_df[[\"epoch\", 'precision','recall','f1-score','f2-score', 'support']].to_csv(f\"{save_dir}/eval_by_epoch\", index=False)\n",
    "print(f\"Model and tokenizer saved to {save_dir}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_envâ€\n\n\nsjupyter\n\n\n\n\n\n\n\nexit\n\n\n[200~ ~dasd\n\nreset\n\nexit()\n\n",
   "language": "python",
   "name": "master_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
