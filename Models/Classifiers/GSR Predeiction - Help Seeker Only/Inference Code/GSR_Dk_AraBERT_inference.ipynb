{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380a18e0-de40-4751-9336-606e0fcf2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, fbeta_score, accuracy_score\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    fbeta_score,\n",
    "    accuracy_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae81e1-b63e-4fb6-9fd2-20a397568195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# load the tokenizer and saved model safetensors\n",
    "model_name = \"aubmindlab/bert-large-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model architecture and weights\n",
    "finetuned_model_path = 'load the path to the folder that includes the model.safetensors'\n",
    "bert_model = BertForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "bert_model.eval()\n",
    "\n",
    "# Start a while loop to check sentences\n",
    "print(\"Enter a sentence to check (or type Q to quit):\")\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"Sentence: \")\n",
    "    if sentence.lower() == 'q':\n",
    "        break\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get the model's predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "        if len(predictions) != 2:\n",
    "            raise Exception(\"Error: the model outputs more than 2 labels, maybe you loaded the wrong model or used wrong architicture\")\n",
    "    # Interpret the result (assuming binary classification: 0 = Not Suicidal, 1 = Suicidal)\n",
    "    result = \"Suicidal\" if prediction == 1 else \"Not Suicidal\"\n",
    "    print(f\"The model predicts: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543e41a-43a9-40a4-b82a-c310fd7f33f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_env‚Äù\n\n\nsjupyter\n\n\n\n\n\n\n\nexit\n\n\n[200~ ~dasd\n\nreset\n\nexit()\n\n",
   "language": "python",
   "name": "master_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
