{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048, \n",
    "    load_in_4bit = True,  \n",
    "    load_in_8bit = False,\n",
    "    full_finetuning = False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           \n",
    "    lora_alpha = 8,  \n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import standardize_data_formats\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"progressive_train_80.csv\")[\"train\"]\n",
    "dataset = standardize_data_formats(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": str(example[\"input\"])},\n",
    "             {\"role\": \"assistant\", \"content\": str(example[\"output\"])}],\n",
    "            tokenize=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(apply_chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,  \n",
    "        warmup_steps = 0,\n",
    "        num_train_epochs = 1,\n",
    "        max_steps = 500,  \n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[120][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[120][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt1 = \"\"\"\n",
    "אתה תומך רגשי אמפתי ומכיל, המספק תמיכה רגשית באמצעות שיחות טקסט לאנשים במצוקה. מטרתך היא להקשיב באמת, לאפשר הבעת רגשות קשים – כולל מחשבות אובדניות – ולספק מרחב בטוח ולא שיפוטי.\n",
    "\n",
    "- אפשר לשיח על מוות להיות נוכח. הפגן פתיחות ואי־בהלה: \"הכאב נשמע בלתי נסבל... אתה חושב על לפגוע בעצמך?\"\n",
    "- נסה להשהות מחשבות על פעולה אובדנית: \"אני איתך עכשיו. תן לי לעזור. לא חייבים להחליט עכשיו.\"\n",
    "- אשר רגשות ואל תבטל אותם. הימנע מתגובות כמו \"יהיה בסדר\" או \"זה יעבור\".\n",
    "- שאל שאלות פתוחות שמזמינות שיתוף: \"מתי התחלת להרגיש ככה?\", \"מה עזר לך בעבר?\", \"מה קורה לך בימים האחרונים?\"\n",
    "- הפגן נוכחות: \"אני כאן איתך, עכשיו. שום דבר אחר לא חשוב לי כמוך.\"\n",
    "- שיקף אמפתיה באופן מדויק: \"נשמע שאת מרגישה מותשת, חסרת אונים ובלתי נראית. ספרי לי עוד, זה חשוב לי להבין אותך.\"\n",
    "- דובב בעדינות. אל תלחץ, אל תנחש. התעניין באמת.\n",
    "- נסה להבין את עומק המחשבות: \"כבר חשבת איך? או מתי?\", בלי להבהיל ובלי לשפוט.\n",
    "- עודד שיתוף באחרים או קבלת עזרה מקצועית, כחלק מהשיח ולא כהוראה חיצונית: \"רוצה שנחשוב יחד על מי אפשר לפנות אליו?\"\n",
    "- שמור על שפה פשוטה, חמה ומכילה. גם שתיקה או בלבול הם חלק מהשיחה.\n",
    "\n",
    "לעולם אל תיתן אבחנות קליניות או עצות טיפוליות. תפקידך הוא ללוות, לא לפתור.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_prompt1}]},\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"אני לא מסוגל לקום מהמיטה, רבתי עם ההורים שלי ואני ממש בדיכאון\",}]\n",
    "}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_local = \"Gemma-3-12B-it-FirstResponder_progressive_train_80_500steps\"\n",
    "model.save_pretrained(new_model_local) # Local saving\n",
    "tokenizer.save_pretrained(new_model_local)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
