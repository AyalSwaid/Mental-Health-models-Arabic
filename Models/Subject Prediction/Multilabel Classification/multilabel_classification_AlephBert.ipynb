{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if using multiple GPUs\n",
    "torch.backends.cudnn.deterministic = True  # Make sure to set this for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Define valid labels for multi-label classification\n",
    "valid_labels = [\"דיכאון ועצבות קשה\", \"פציעה עצמית\", \"טראומה מינית\", \"Other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "model_name = 'onlplab/alephbert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(valid_labels))  # num_labels = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Data\n",
    "conv_info_path = '/home/astrin/Projects/Sahar/Dataset/conv_info.csv'\n",
    "messages_path = '/home/astrin/Projects/Sahar/Dataset/messages_anonymized.csv'\n",
    "\n",
    "conv_info_df = pd.read_csv(conv_info_path)\n",
    "messages_df = pd.read_csv(messages_path)\n",
    "\n",
    "conv_info_df['engagement_id'] = conv_info_df['engagement_id'].astype(str)\n",
    "messages_df['engagement_id'] = messages_df['engagement_id'].astype(str)\n",
    "messages_df = messages_df[messages_df['text'].notna()]\n",
    "messages_df['name'] = messages_df['name'].fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-label binary representation for each conversation including \"Other\"\n",
    "def assign_multi_labels(row):\n",
    "    labels = []\n",
    "    for label in valid_labels[:-1]:  # Exclude \"Other\" at first\n",
    "        if label in [row['subject_1'], row['subject_2'], row['subject_3']]:\n",
    "            labels.append(label)\n",
    "    # If no valid labels found, append \"Other\"\n",
    "    if not labels:\n",
    "        labels.append(\"Other\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign multi-labels (list of labels) for each row\n",
    "conv_info_df['labels'] = conv_info_df.apply(assign_multi_labels, axis=1)\n",
    "\n",
    "# Merge messages and conversation info\n",
    "merged_df = messages_df.merge(conv_info_df, on='engagement_id')\n",
    "\n",
    "# Filter only the help seeker's messages\n",
    "merged_df = merged_df[merged_df['seeker'] == True]\n",
    "\n",
    "# Aggregate the messages so each row contains an entire conversation\n",
    "merged_df = merged_df.groupby('engagement_id').agg({'text': '[SEP]'.join, 'labels': 'first'}).reset_index()\n",
    "\n",
    "# Use MultiLabelBinarizer to convert the labels into binary vectors\n",
    "mlb = MultiLabelBinarizer(classes=valid_labels)\n",
    "merged_df['label'] = mlb.fit_transform(merged_df['labels']).tolist()\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30232\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf01a93981db4f87afab87f9ac5818e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24185 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4b2232ab324c80be39a3d0fb975e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6047 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=16)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=16)\n",
    "\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 24185\n",
      "Test set size: 6047\n",
      "label\n",
      "[0, 0, 0, 1]    3784\n",
      "[1, 0, 0, 0]    1620\n",
      "[0, 0, 1, 0]     282\n",
      "[0, 1, 0, 0]     161\n",
      "[1, 1, 0, 0]     110\n",
      "[1, 0, 1, 0]      73\n",
      "[0, 1, 1, 0]      15\n",
      "[1, 1, 1, 0]       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check size of train and test sets\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Check the distribution of labels in the test set\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0c3132ea284ec98304e1671d558714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the loss function explicitly as BCEWithLogitsLoss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Model Training\n",
    "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=learning_rate)\n",
    "bert_model.train()\n",
    "\n",
    "progress_bar = tqdm(range(epochs * len(train_loader)), desc=\"Training\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = torch.tensor(batch['label'], dtype=torch.float32).to(device)  # Binary labels\n",
    "\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits  # Get the logits from the model outputs\n",
    "\n",
    "        # Calculate the loss using BCEWithLogitsLoss\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb2eb8e253f4c63bd3a7889a89604ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Evaluating the Model\n",
    "bert_model.eval()\n",
    "labels = []\n",
    "preds = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    label = torch.tensor(batch['label'], dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.sigmoid(logits)  # Use sigmoid for multi-label classification\n",
    "\n",
    "    # Apply threshold to predictions (0.5 by default for binary classification)\n",
    "    predictions = (predictions > 0.5).int()\n",
    "\n",
    "    labels.extend(label.cpu().numpy())\n",
    "    preds.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert the predictions and true labels to numpy arrays\n",
    "labels = np.array(labels)\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>דיכאון ועצבות קשה</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.616620</td>\n",
       "      <td>0.626337</td>\n",
       "      <td>1805.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>פציעה עצמית</th>\n",
       "      <td>0.622581</td>\n",
       "      <td>0.670139</td>\n",
       "      <td>0.645485</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>טראומה מינית</th>\n",
       "      <td>0.845833</td>\n",
       "      <td>0.545699</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.822128</td>\n",
       "      <td>0.820825</td>\n",
       "      <td>0.821476</td>\n",
       "      <td>3784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.759421</td>\n",
       "      <td>0.738518</td>\n",
       "      <td>0.748824</td>\n",
       "      <td>6249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.731726</td>\n",
       "      <td>0.663321</td>\n",
       "      <td>0.689174</td>\n",
       "      <td>6249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.760685</td>\n",
       "      <td>0.738518</td>\n",
       "      <td>0.747589</td>\n",
       "      <td>6249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.743895</td>\n",
       "      <td>0.743592</td>\n",
       "      <td>0.740985</td>\n",
       "      <td>6249.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   precision    recall  f1-score  support\n",
       "דיכאון ועצבות קשה   0.636364  0.616620  0.626337   1805.0\n",
       "פציעה עצמית         0.622581  0.670139  0.645485    288.0\n",
       "טראומה מינית        0.845833  0.545699  0.663399    372.0\n",
       "Other               0.822128  0.820825  0.821476   3784.0\n",
       "micro avg           0.759421  0.738518  0.748824   6249.0\n",
       "macro avg           0.731726  0.663321  0.689174   6249.0\n",
       "weighted avg        0.760685  0.738518  0.747589   6249.0\n",
       "samples avg         0.743895  0.743592  0.740985   6249.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Classification Report for Multi-label\n",
    "report_dict = classification_report(labels, preds, target_names=valid_labels, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# # Export the classification report to Excel\n",
    "file_path = \"multilabel_classification_with_other_report.xlsx\"\n",
    "report_df.to_excel(file_path)\n",
    "\n",
    "# Display the DataFrame to view the classification report in a tabular format\n",
    "report_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
